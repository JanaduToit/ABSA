{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\jduto\\AppData\\Local\\Temp\\ipykernel_20052\\167770346.py:386: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  s['clean_text'] = s.apply(preprocess_text)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total Vocabulary Size: 2175\n",
      "Total Vocabulary Size: 2175\n",
      "Total Vocabulary Size: 2175\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\jduto\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "sns.set()\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from collections import Counter\n",
    "import nltk\n",
    "from nltk.tokenize import sent_tokenize, word_tokenize, RegexpTokenizer, ToktokTokenizer\n",
    "import re\n",
    "from nltk import pos_tag\n",
    "from wordcloud import WordCloud\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import classification_report, confusion_matrix, accuracy_score, recall_score, precision_score, f1_score\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from string import punctuation\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "import os\n",
    "from spellchecker import SpellChecker\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "\n",
    "pd.set_option('display.float_format', lambda x: '%.3f' % x)\n",
    "import pyLDAvis\n",
    "import pyLDAvis.gensim\n",
    "import spacy\n",
    "import gensim\n",
    "import gensim.corpora as corpora\n",
    "from gensim.utils import simple_preprocess\n",
    "from gensim.models import CoherenceModel\n",
    "\n",
    "import re\n",
    "from spellchecker import SpellChecker\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "\n",
    "data_2020 = pd.read_excel(r\"C:\\Users\\jduto\\OneDrive\\Documents\\Meesters\\Coding\\University Data\\Statistics-SFTS-qualitative-data-in-2020-Report.xlsx\")\n",
    "data_2021 = pd.read_excel(r\"C:\\Users\\jduto\\OneDrive\\Documents\\Meesters\\Coding\\University Data\\Statistics-SFTS-ENDQs_qualitative-data-2021-Report.xlsx\")\n",
    "data_2022 = pd.read_excel(r\"C:\\Users\\jduto\\OneDrive\\Documents\\Meesters\\Coding\\University Data\\Statistics-SFTS-Qualitative-ENDQs-2022-Report.xlsx\")\n",
    "data_2023 = pd.read_excel(r\"C:\\Users\\jduto\\OneDrive\\Documents\\Meesters\\Coding\\University Data\\Statistics-SFTS-Qualtitative-ENDQ1&2_2023-Report.xlsx\")\n",
    "# Concatenate by rows\n",
    "result_df = pd.concat([data_2020, data_2021, data_2022, data_2023], ignore_index=True)\n",
    "# Assuming result_df is your concatenated DataFrame\n",
    "result = result_df.iloc[:, :-3]\n",
    "data = result.iloc[:, :8].join(result.iloc[:,-1], how='inner')\n",
    "import pandas as pd\n",
    "\n",
    "# Assuming 'data' is your DataFrame\n",
    "new_data = []\n",
    "\n",
    "for index, row in data.iterrows():\n",
    "    course_code = row['Course']\n",
    "    semester = row['Semester']\n",
    "    year = row['Year']\n",
    "    question_key = row['QuestionKey']\n",
    "    \n",
    "    # Check if 'Comments' is not NaN\n",
    "    if pd.notna(row['Comments']):\n",
    "        comments = row['Comments'].split('||')\n",
    "\n",
    "        for comment in comments:\n",
    "            new_data.append({'Course': course_code,'Semester':semester,'Year':year,'QuestionKey': question_key ,'Comments': comment.strip()})\n",
    "new_df = pd.DataFrame(new_data)\n",
    "\n",
    "import re\n",
    "from collections import Counter\n",
    "import pandas as pd\n",
    "\n",
    "def find_capital_words(text):\n",
    "    if pd.isna(text) or not isinstance(text, str):\n",
    "        return []  # Return an empty list for non-string or missing values\n",
    "    \n",
    "    pattern = r'\\b[A-Z][a-z]*\\b'\n",
    "    capital_words = re.findall(pattern, text)\n",
    "    return capital_words\n",
    "\n",
    "# Apply the function to the 'Comments' column\n",
    "new_df['Capitalized_Words'] = new_df['Comments'].apply(find_capital_words)\n",
    "\n",
    "# Flatten the lists and count the occurrences\n",
    "all_capital_words = [word for words_list in new_df['Capitalized_Words'] for word in words_list]\n",
    "word_counts = Counter(all_capital_words)\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from collections import Counter\n",
    "import nltk\n",
    "from nltk.tokenize import sent_tokenize, word_tokenize, RegexpTokenizer, ToktokTokenizer\n",
    "import re\n",
    "from nltk import pos_tag\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import classification_report, confusion_matrix, accuracy_score, recall_score, precision_score, f1_score\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from string import punctuation\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "import os\n",
    "from spellchecker import SpellChecker\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "\n",
    "import re\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize\n",
    "\n",
    "s = new_df['Comments']\n",
    "tokenizer = ToktokTokenizer()\n",
    "stopword_list = nltk.corpus.stopwords.words('english')\n",
    "tokenizer = word_tokenize\n",
    "custom_spellchecker = SpellChecker()\n",
    "\n",
    "# Contractions dictionary\n",
    "contractions_dict = {\n",
    "\"tuts\":\"tutorial\",\n",
    "\"pracs\":\"practical\",\n",
    "\"labs\":\"laboratory\",\n",
    "\"imroove\":\"improve\",\n",
    "\"preparfed\":\"prepared\",\n",
    "\"difficultto\":\"difficult to\",\n",
    "\"understadable\":\"understandable\",\n",
    "\"verry\":\"very\",\n",
    "\"mpdules\":\"module\",\n",
    "\"challanging\":\"challenging\",\n",
    "\"questionair\":\"questionnaire\",\n",
    "\"affraid\":\"afraid\",\n",
    "\"interacte\":\"interact\",\n",
    "\"semeter\":\"semester\",\n",
    "\"ive\":\"i have\",\n",
    "\"werent\":\"were not\",\n",
    "\"preform\":\"perform\",\n",
    "\"toget\":\"to get\",\n",
    "\"hjave\":\"have\",\n",
    "\"turioals\":\"tutorial\",\n",
    "\"detial\":\"detail\",\n",
    "\"problemes\":\"problem\",\n",
    "\"lucturer\":\"lecturer\",\n",
    "\"lactures\":\"lecture\",\n",
    "\"sloving\":\"solving\",\n",
    "\"begining\":\"beginning\",\n",
    "\"activly\":\"actively\",\n",
    "\"benefeicial\":\"beneficial\",\n",
    "\"struggeling\":\"struggl\",\n",
    "\"acessable\":\"accessible\",\n",
    "\"undersanding\":\"understand\",\n",
    "\"absolutley\":\"absolutely\",\n",
    "\"connivence\":\"convenience\",\n",
    "\"acces\":\"access\",\n",
    "\"inn\":\":in\",\n",
    "\"asses\":\"assess\",\n",
    "\"imformative\":\"informative\",\n",
    "\"leraning\":\"learn\",\n",
    "\"schdules\":\"schedule\",\n",
    "\"convinient\":\"convenient\",\n",
    "\"exmaples\":\"example\",\n",
    "\"explaing\":\"explaining\",\n",
    "\"blckboard\":\"blackboard\",\n",
    "\"collabarate\":\"collaborate\",\n",
    "\"treffic\":\"traffic\",\n",
    "\"materals\":\"material\",\n",
    "\"reallu\":\"really\",\n",
    "\"derivati\":\"derivative\",\n",
    "\"subjetcs\":\"subject\",\n",
    "\"blackbord\":\"blackboard\",\n",
    "\"resorces\":\"resource\",\n",
    "\"thurday\":\"thursday\",\n",
    "\"reaources\":\"resource\",\n",
    "\"traveltime\":\"travel time\",\n",
    "\"comute\":\"commute\",\n",
    "\"enoy\":\"enjoy\",\n",
    "\"expatiation\":\"explanation\",\n",
    "\"hvaing\":\"having\",\n",
    "\"pase\":\"phase\",\n",
    "\"lecturere\":\"lecturer\",\n",
    "\"untill\":\"until\",\n",
    "\"wasy\":\"easy\",\n",
    "\"progam\":\"program\",\n",
    "\"atrending\":\"attending\",\n",
    "\"jy\":\"you\",\n",
    "\"querry\":\"query\",\n",
    "\"mnay\":\"may\",\n",
    "\"wprk\":\"work\",\n",
    "\"feedbac\":\"feedback\",\n",
    "\"addedd\":\"added\",\n",
    "\"somethigs\":\"something\",\n",
    "\"empithy\":\"empathy\",\n",
    "\"doin\":\"doing\",\n",
    "\"communiation\":\"communication\",\n",
    "\"diffuclt\":\"difficult\",\n",
    "\"ons\":\"us\",\n",
    "\"unfortunatly\":\"unfortunately\",\n",
    "\"unnecassary\":\"unnecessary\",\n",
    "\"expeienced\":\"experience\",\n",
    "\"adresssed\":\"addressed\",\n",
    "\"loadshadding\":\"loadshedding\",\n",
    "\"inish\":\"finish\",\n",
    "\"lotttttt\":\"lot\",\n",
    "\"manyassignments\":\"many assignment\",\n",
    "\"conducti\":\"conduct\",\n",
    "\"upconect\":\"upconnect\",\n",
    "\"loadsehedding\":\"loadshedding\",\n",
    "\"asistance\":\"assistance\",\n",
    "\"statisitics\":\"statistics\",\n",
    "\"ohysical\":\"physical\",\n",
    "\"noy\":\"not\",\n",
    "\"slowely\":\"slowly\",\n",
    "\"sceduling\":\"schedule\",\n",
    "\"snd\":\"and\",\n",
    "\"explanatius\":\"explanation\",\n",
    "\"classess\":\"class\",\n",
    "\"doingg\":\"doing\",\n",
    "\"cusultation\":\"consultation\",\n",
    "\"explanation\":\"explanation\",\n",
    "\"questius\":\"question\",\n",
    "\"sessius\":\"session\",\n",
    "\"cusultatius\":\"consultation\",\n",
    "\"activities\":\"activity\",\n",
    "\"papers\":\"paper\",\n",
    "\"homeworks\":\"homework\",\n",
    "\"consulations\":\"consultation\",\n",
    "\"assessments\":\"assessment\",\n",
    "\"treffic\":\"traffic\",\n",
    "\"fir\":\"for\",\n",
    "\"kinda\":\"kind of\",\n",
    "\"struggl\":\"struggle\",\n",
    "\"alot\":\"a lot\"\n",
    "}\n",
    "\n",
    "sigular_dict = {\n",
    "    \"classes\":\"class\",\n",
    "    \"tests\":\"test\",\n",
    "    \"examples\":\"example\",\n",
    "    \"methods\":\"method\",\n",
    "    \"lectures\":\"lecture\",\n",
    "    \"lecturers\":\"lecturer\",\n",
    "    \"homeworks\":\"homework\",\n",
    "    \"helps\":\"help\",\n",
    "    \"activities\":\"activity\",\n",
    "    \"concepts\":\"concept\",\n",
    "    \"worked\":\"work\",\n",
    "    \"slides\":\"slide\",\n",
    "    \"uploaded\":\"upload\",\n",
    "    \"exercises\":\"exercise\",\n",
    "    \"questions\":\"question\",\n",
    "    \"answers\":\"answer\",\n",
    "    \"conclusions\":\"conclusion\",\n",
    "    \"datasets\":\"dataset\",\n",
    "    \"grades\":\"grade\",\n",
    "    \"tips\":\"tip\",\n",
    "    \"tricks\":\"trick\",\n",
    "    \"demands\":\"demand\",\n",
    "    \"pretests\":\"pretest\",\n",
    "    \"voices\":\"voice\",\n",
    "    \"dislikes\":\"dislike\",\n",
    "    \"concerns\":\"concern\",\n",
    "    \"reads\":\"read\",\n",
    "    \"benefited\":\"benefit\",\n",
    "    \"tasks\":\"task\",\n",
    "    \"papers\":\"paper\",\n",
    "    \"explanations\":\"explanation\",\n",
    "    \"notes\":\"note\",\n",
    "    \"dished\":\"dish\",\n",
    "    \"videos\":\"video\",\n",
    "    \"uploaded\":\"upload\",\n",
    "    \"sheets\":\"sheet\",\n",
    "    \"topics\":\"topic\",\n",
    "    \"books\":\"book\",\n",
    "    \"needed\":\"need\",\n",
    "    \"exams\":\"exam\",\n",
    "    \"cramming\":\"cram\",\n",
    "    \"summaries\":\"summary\",\n",
    "    \"proofs\":\"proof\",\n",
    "    \"laptops\":\"laptop\",\n",
    "    \"devices\":\"device\",\n",
    "    \"trees\":\"tree\",\n",
    "    \"tutors\":\"tutor\",\n",
    "    \"assignments\":\"assignment\",\n",
    "    \"worksheets\":\"worksheet\",\n",
    "    \"aspects\":\"aspect\",\n",
    "    \"students\":\"student\",\n",
    "    \"documents\":\"document\",\n",
    "    \"practicals\":\"practical\",\n",
    "    \"difficulties\":\"difficult\",\n",
    "    \"modules\":\"module\",\n",
    "    \"probabilities\":\"probability\",\n",
    "    \"matrices\":\"matrix\",\n",
    "    \"derivatives\":\"derivative\",\n",
    "    \"attempts\":\"attempt\",\n",
    "    \"mistakes\":\"mistake\",\n",
    "    \"tested\":\"test\",\n",
    "    \"dealt\":\"deal\",\n",
    "    \"deals\":\"deal\",\n",
    "    \"marks\":\"mark\",\n",
    "    \"skips\":\"skip\",\n",
    "    \"chapters\":\"chapter\",\n",
    "    \"years\":\"year\",\n",
    "    \"submissions\":\"submission\",\n",
    "    \"skills\":\"skill\",\n",
    "    \"provided\":\"provide\",\n",
    "    \"sildes\":\"slide\",\n",
    "    \"works\":\"work\",\n",
    "    \"problems\":\"problem\",\n",
    "    \"times\":\"time\",\n",
    "    \"forms\":\"form\",\n",
    "    \"connections\":\"connection\",\n",
    "    \"consultations\":\"consultation\",\n",
    "    \"sessions\":\"session\",\n",
    "    \"makes\":\"make\",\n",
    "    \"paced\":\"pace\",\n",
    "    \"forces\":\"force\",\n",
    "    \"weeks\":\"week\",\n",
    "    \"recorded\":\"record\",\n",
    "    \"studies\":\"study\",\n",
    "    \"cases\":\"case\",\n",
    "    \"struggl\":\"struggle\",\n",
    "    \"prac\":\"practical\",\n",
    "    \"saves\":\"save\",\n",
    "    \"pracs\":\"practical\",\n",
    "    \"practiced\":\"practise\",\n",
    "    \"practised\":\"practise\",\n",
    "    \"notational\":\"notation\",\n",
    "    \"oher\":\"other\",\n",
    "    \"oay\":\"okay\",\n",
    "    \"assessed\":\"assess\",\n",
    "    \"comments\":\"comment\",\n",
    "    \"feels\":\"feel\",\n",
    "    \"miised\":\"miss\",\n",
    "    \"communicates\":\"communicate\",\n",
    "    \"presentations\":\"presentation\",\n",
    "    \"pausing\":\"pause\",\n",
    "    \"moderators\":\"moderator\",\n",
    "    \"assigments\":\"assignment\",\n",
    "    \"communicating\":\"communicate\",\n",
    "    \"tutorials\":\"tutorial\"\n",
    "}\n",
    "contractions_re = re.compile('(%s)' % '|'.join(contractions_dict.keys()))\n",
    "\n",
    "def expand_contractions(text, contractions_dict=contractions_dict):\n",
    "    def replace(match):\n",
    "        return contractions_dict[match.group(0)]\n",
    "    return contractions_re.sub(replace, text)\n",
    "\n",
    "def correct_spelling(text, corrections_dict=contractions_dict):\n",
    "    words = text.split()\n",
    "    corrected_words = [corrections_dict.get(word, word) for word in words]\n",
    "    corrected_text = ' '.join(corrected_words)\n",
    "    return corrected_text\n",
    "\n",
    "def remove_special_characters(text, remove_digits=True):\n",
    "    pattern = r'[^a-zA-Z0-9\\s]' if not remove_digits else r'[^a-zA-Z\\s]'\n",
    "    text = re.sub(pattern, ' ', text)\n",
    "    return text\n",
    "\n",
    "def remove_stopwords(text, is_lower_case=False):\n",
    "    tokens = word_tokenize(text)\n",
    "    tokens = [token.strip() for token in tokens]\n",
    "    if is_lower_case:\n",
    "        filtered_tokens = [token for token in tokens if token not in stopword_list]\n",
    "    else:\n",
    "        filtered_tokens = [token for token in tokens if token.lower() not in stopword_list]\n",
    "    filtered_text = ' '.join(filtered_tokens)\n",
    "    return filtered_text\n",
    "\n",
    "def singularize_words(text, singular_dict):\n",
    "    words = text.split()\n",
    "    singularized_words = [singular_dict.get(word, word) for word in words]\n",
    "    singularized_text = ' '.join(singularized_words)\n",
    "    return singularized_text\n",
    "\n",
    "\n",
    "def preprocess_text(text):\n",
    "    text = text.lower()\n",
    "    text = correct_spelling(text)\n",
    "    text = remove_special_characters(text)\n",
    "    #remove stop words\n",
    "    additional_words = ['da','terminator','ms','pace','ta','ac','za','loqd','dis','trst','cbts','itsi','miebooks','ya','ms','neethlings','neethling','dr','wah','kleyn','e','na','ie','isnt','miss ','ane','cap','stk','sfts','burger','kraamwinkel','inger','fletcher','prof','pretoria','university','lindo','christene','n','nothing','none','bcom','niks','comment','non','applicable','u','z','nagar','bme','wst','stc','acc','sci','up','llb','stem','sir','ntn','mrs','le','coster','madam','usualky','thou','couldve','muslim','cultures','ramadan','preparfed','nina','masters','difficultto','tukstube','sssignments','actuary','phd','memose','trecky','wendy','ozark','thuto','najmeh','pedagogy','understadable','understandng','verry','liberating','calcute','undesrtandable','ams','kraamwinkle','mr','magagula','zl','coz','b','retief',  'makgai','cleo','fabris','rotelli','tuks','sir','hatfield','swaziland','eswatini','fletcher','google','wikipedeia','qwaqwa','lusaka','south','africa','merensky','zoom','res','wi','fi','minister','residence','swanepoel','universities','judy','paul','van','staden','ehlers','ehler','doctor','pearson','christine','khan','york','mars','hackettstown','bayer','ferreria',\n",
    "                        'mam','crafford','merwe','mister','priyanka','didi','salomi','sirs','lindi','mahloromela','otieno','arnold','stander','fereirra','ferrreira','luke','farreira','morden','cambridge','rene','seite','mrs','michelle','klerk','numbas','divan','mamelodi','maribe','iketle','maharela','reyneke','coetser','limpopo','ratief','rian','waal','de','botes','jocelyn',\n",
    "                        'mazarura','ansie','smit','kabelo','derks','iena','dirks','ferreira','johan','renate','thiede','gao','skhosana','coester','cloester','arminn','brenda','malela','oduol','chimamanda','ngozi','adichie','tristan','lebogang','evernice','fletchers','macdoul','alexander','kelbrick','loina','linda']\n",
    "    stopwords_list = \"you'll,mustn't,it's,mustn,hasn't,didn't,needn't,should've,shouldn't,isn't,you're,that'll,wasn't,couldn't,shan,you've,shan't,doesn't,ma,mightn,wouldn't,won't,haven't,you'd,hadn't,y,weren't,mightn't,needn,don't,o,aren't,she's,u,a,s,able,about,above,according,accordingly,across,actually,after,afterwards,again,against,ain,t,all,allow,allows,almost,alone,along,already,also,although,always,am,among,amongst,an,and,another,any,anybody,anyhow,anyone,anything,anyway,anyways,anywhere,apart,appear,appreciate,appropriate,are,aren,t,around,as,aside,ask,asking,associated,at,available,away,awfully,be,became,because,become,becomes,becoming,been,before,beforehand,behind,being,believe,below,beside,besides,best,better,between,beyond,both,brief,but,by,c,mon,c,s,came,can,can,t,cannot,cant,cause,causes,certain,certainly,changes,clearly,co,com,come,comes,concerning,consequently,consider,considering,contain,containing,contains,corresponding,could,couldn,t,course,currently,definitely,described,despite,did,didn,t,different,do,does,doesn,t,doing,don,t,done,down,downwards,during,each,edu,eg,eight,either,else,elsewhere,enough,entirely,especially,et,etc,even,ever,every,everybody,everyone,everything,everywhere,ex,exactly,example,except,far,few,fifth,first,five,followed,following,follows,for,former,formerly,forth,four,from,further,furthermore,get,gets,getting,given,gives,go,goes,going,gone,got,gotten,greetings,had,hadn,t,happens,hardly,has,hasn,t,have,haven,t,having,he,he,s,hello,help,hence,her,here,here,s,hereafter,hereby,herein,hereupon,hers,herself,hi,him,himself,his,hither,http,hopefully,how,howbeit,however,i,d,i,ll,i,m,i,ve,ie,if,ignored,immediate,in,inasmuch,inc,indeed,indicate,indicated,indicates,inner,insofar,instead,into,inward,is,isn,t,it,it,d,it,ll,it,s,its,itself,just,keep,keeps,kept,know,knows,known,last,lately,later,latter,latterly,least,less,lest,let,let,s,like,liked,likely,little,look,looking,looks,ltd,mainly,many,may,maybe,me,mean,meanwhile,merely,might,more,moreover,most,mostly,much,must,my,myself,name,namely,nd,near,nearly,necessary,need,needs,neither,never,nevertheless,new,next,nine,no,nobody,non,none,noone,nor,normally,not,nothing,novel,now,nowhere,obviously,of,off,often,oh,ok,okay,old,on,once,one,ones,only,onto,or,other,others,otherwise,ought,our,ours,ourselves,out,outside,over,overall,own,particular,particularly,per,perhaps,placed,please,plus,possible,presumably,probably,provides,proftimnoakes,que,quite,qv,rather,rd,re,really,reasonably,regarding,regardless,regards,relatively,respectively,right,said,same,saw,say,saying,says,second,secondly,see,seeing,seem,seemed,seeming,seems,seen,self,selves,sensible,sent,serious,seriously,seven,several,shall,she,should,shouldn,t,since,six,so,some,somebody,somehow,someone,something,sometime,sometimes,somewhat,somewhere,soon,sorry,specified,specify,specifying,still,sub,such,sup,sure,t,s,take,taken,tell,tends,th,than,thank,thanks,thanx,that,that,s,thats,the,their,theirs,them,themselves,then,thence,there,there,s,thereafter,thereby,therefore,therein,theres,thereupon,these,they,they,d,they,ll,they,re,rt,they,ve,think,third,this,thorough,thoroughly,those,though,three,through,throughout,thru,thus,to,together,too,took,toward,towards,tried,tries,truly,try,trying,twice,two,un,under,unfortunately,unless,unlikely,until,unto,up,upon,us,use,used,useful,uses,using,usually,value,various,very,via,viz,vs,want,wants,was,wasn,t,way,we,we,d,we,ll,we,re,we,ve,welcome,well,went,were,weren,t,what,what,s,whatever,when,whence,whenever,where,where,s,whereafter,whereas,whereby,wherein,whereupon,wherever,whether,which,while,whither,who,who,s,whoever,whole,whom,whose,why,will,willing,wish,with,within,without,won,t,wonder,would,would,wouldn,t,yes,yet,you,you,d,you,ll,you,re,you,ve,your,yours,yourself,yourselves,zero\".split(',')\n",
    "\n",
    "    text = \" \".join([word for word in str(text).split() if word not in stopwords_list])\n",
    "    text = \" \".join([word for word in str(text).split() if word not in additional_words])\n",
    "    text = singularize_words(text,sigular_dict)\n",
    "    return text\n",
    "s['clean_text'] = s.apply(preprocess_text)\n",
    "\n",
    "data = list(s['clean_text'])\n",
    "bigram = gensim.models.Phrases(data, min_count=20, threshold=100) \n",
    "trigram = gensim.models.Phrases(bigram[data], threshold=100)  \n",
    "\n",
    "bigram_mod = gensim.models.phrases.Phraser(bigram)\n",
    "trigram_mod = gensim.models.phrases.Phraser(trigram)\n",
    "\n",
    "nlp = spacy.load('en_core_web_sm', disable=['parser', 'ner'])\n",
    "\n",
    "f = [\"friend\",\"list\",\"man\",\"woman\",\"husband\",\"wife\",\"boyfriend\",\"girlfriend\",\"sake\",'lot',\"way\",\"choice\",\"minute\",\"def\",\"stuff\",\"fact\",\"app\",\"bet\",\"thing\"]\n",
    "stop_words = nltk.corpus.stopwords.words('english') \n",
    "def process_words(texts, stop_words=stop_words, allowed_tags=['NOUN']):\n",
    "    \n",
    "    \"\"\"Convert a document into a list of lowercase tokens, build bigrams-trigrams, implement lemmatization\"\"\"\n",
    "    \n",
    "    # remove stopwords, short tokens and letter accents \n",
    "    texts = [[word for word in simple_preprocess(str(doc), deacc=True, min_len=3) if word not in stop_words] for doc in texts]\n",
    "    \n",
    "    # bi-gram and tri-gram implementation\n",
    "    texts = [bigram_mod[doc] for doc in texts]\n",
    "    texts = [trigram_mod[bigram_mod[doc]] for doc in texts]\n",
    "    \n",
    "    texts_out = []\n",
    "\n",
    "    # implement lemmatization and filter out unwanted part of speech tags\n",
    "    for sent in texts:\n",
    "        doc = nlp(\" \".join(sent)) \n",
    "        texts_out.append([token.lemma_ for token in doc if token.pos_ in allowed_tags])\n",
    "    \n",
    "    # remove stopwords and short tokens again after lemmatization\n",
    "    texts_out = [[word for word in simple_preprocess(str(doc), deacc=True, min_len=3) if word not in stop_words] for doc in texts_out]    \n",
    "    \n",
    "    return texts_out\n",
    "     \n",
    "    \n",
    "data_ready = process_words(data)  \n",
    "# Create Dictionary\n",
    "id2word = corpora.Dictionary(data_ready)\n",
    "\n",
    "print('Total Vocabulary Size:', len(id2word))\n",
    "# Create Corpus: Term Document Frequency\n",
    "corpus = [id2word.doc2bow(text) for text in data_ready]\n",
    "\n",
    "dict_corpus = {}\n",
    "\n",
    "for i in range(len(corpus)):\n",
    "  for idx, freq in corpus[i]:\n",
    "    if id2word[idx] in dict_corpus:\n",
    "      dict_corpus[id2word[idx]] += freq\n",
    "    else:\n",
    "       dict_corpus[id2word[idx]] = freq\n",
    "\n",
    "    \n",
    "dict_df = pd.DataFrame.from_dict(dict_corpus, orient='index', columns=['freq'])\n",
    "#extension = dict_df[dict_df.freq<5].index.tolist()\n",
    "#stop_words.extend(extension)\n",
    "\n",
    "data_ready = process_words(data)\n",
    "# Create Dictionary\n",
    "id2word = corpora.Dictionary(data_ready)\n",
    "\n",
    "print('Total Vocabulary Size:', len(id2word))\n",
    "     \n",
    "# Filter out words that occur less than 10 documents, or more than 50% of the documents.\n",
    "\n",
    "#id2word.filter_extremes(no_below=10, no_above=0.8)\n",
    "\n",
    "print('Total Vocabulary Size:', len(id2word))\n",
    "\n",
    "# Create Corpus: Term Document Frequency\n",
    "corpus = [id2word.doc2bow(text) for text in data_ready]\n",
    "\n",
    "from gensim.models import LdaModel\n",
    "\n",
    "# Specify the path to the Mallet binary\n",
    "\n",
    "# Train the LDA model using LdaModel with MalletCorpus\n",
    "ldamodel = LdaModel(\n",
    "    corpus=corpus,\n",
    "    id2word=id2word,\n",
    "    num_topics=100\n",
    ")\n",
    "\n",
    "words = list(id2word.values())\n",
    "nums = list(id2word.keys())\n",
    "stacked = s['clean_text']\n",
    "big_list_of_sentence_topics = []\n",
    "for i in stacked.T:\n",
    "    sentence_topics = []\n",
    "    tokens = i.split()\n",
    "    for k in tokens:\n",
    "        if k in stop_words : continue\n",
    "        try:\n",
    "            position = words.index(k)\n",
    "            key = nums[position]\n",
    "            sentence_topics.append(key)\n",
    "        except ValueError: continue\n",
    "    big_list_of_sentence_topics.append(sentence_topics)\n",
    "\n",
    "test_sentence = big_list_of_sentence_topics[1]\n",
    "for i in test_sentence:\n",
    "    words = nums.index(i)\n",
    "def get_sentence_topics(sentence):\n",
    "    topic_list = []\n",
    "    for i in sentence:\n",
    "        topic_prob = ldamodel.get_term_topics(word_id=i, minimum_probability = 0.0)\n",
    "        topic_list.append(topic_prob)\n",
    "    return topic_list\n",
    "obj = get_sentence_topics(big_list_of_sentence_topics[1])\n",
    "test_df = pd.DataFrame(obj)\n",
    "test_df = pd.DataFrame(test_df.stack())\n",
    "index_list = []\n",
    "for i in range(len(test_df)):\n",
    "    counter = i\n",
    "    index_list.append(counter)\n",
    "    counter+=1\n",
    "#print (index_list)\n",
    "test_df.index = index_list\n",
    "\n",
    "def predict_top_topic(obj):\n",
    "    temp_df = pd.DataFrame(obj)\n",
    "    temp_df = temp_df.stack()\n",
    "    categories_list = []\n",
    "    prob_list = []\n",
    "    index_list = []\n",
    "    for i in range(len(temp_df)):\n",
    "        counter = i\n",
    "        index_list.append(counter)\n",
    "        counter+=1\n",
    "    temp_df.index = index_list\n",
    "    for i in temp_df.T:\n",
    "        categories_list.append(i[0])\n",
    "        prob_list.append(i[1])\n",
    "    categories_df = pd.DataFrame(categories_list)\n",
    "    col_name = ['cat']\n",
    "    categories_df.columns = col_name\n",
    "    categories_df['probs'] = pd.DataFrame(prob_list)\n",
    "    categories_df = categories_df.groupby(['cat']).sum()\n",
    "    top = categories_df.sort_values(by=['probs'], ascending = False)\n",
    "    top = top.head(1).index[0]\n",
    "    return top\n",
    "\n",
    "err_list = []\n",
    "pred_list = []\n",
    "for i in range(len(big_list_of_sentence_topics)):\n",
    "    obj = get_sentence_topics(big_list_of_sentence_topics[i])\n",
    "    try:\n",
    "        pred = predict_top_topic(obj)\n",
    "        pred_list.append(pred)\n",
    "    except ValueError:\n",
    "            err_list.append(big_list_of_sentence_topics[i])\n",
    "            #print('Error at position: ',i)\n",
    "            pred_list.append(999)\n",
    "\n",
    "pred_df = pd.DataFrame(pred_list)\n",
    "counts = pred_df.groupby(0).sum()\n",
    "col = ['topic']\n",
    "pred_df.columns = col\n",
    "\n",
    "a = {}\n",
    "num_clusters =100  # Adjust the number of clusters as needed\n",
    "for i in range(num_clusters):\n",
    "    a[i] = (stacked[pred_df.topic == i])\n",
    "    \n",
    "# Text Summarization\n",
    "nltk.download('punkt')\n",
    "stop_words = set(stopwords.words('english'))\n",
    "\n",
    "# Convert the sentences in a0 to a single string\n",
    "\n",
    "\n",
    "def generate_summary_from_text(text, top_n=5):\n",
    "    sentences = sent_tokenize(text)\n",
    "    \n",
    "    # Preprocess the sentences\n",
    "    sentences = [sentence.lower() for sentence in sentences]\n",
    "    sentences = [word_tokenize(sentence) for sentence in sentences]\n",
    "    sentences = [[word for word in sentence if word.isalnum()] for sentence in sentences]\n",
    "    \n",
    "    # Remove stop words\n",
    "    sentences = [[word for word in sentence if word not in stop_words] for sentence in sentences]\n",
    "    \n",
    "    # Calculate sentence scores based on word frequencies\n",
    "    word_frequencies = {}\n",
    "    for sentence in sentences:\n",
    "        for word in sentence:\n",
    "            if word not in word_frequencies:\n",
    "                word_frequencies[word] = 1\n",
    "            else:\n",
    "                word_frequencies[word] += 1\n",
    "    \n",
    "    # Normalize the word frequencies\n",
    "    maximum_frequency = max(word_frequencies.values())\n",
    "    for word in word_frequencies.keys():\n",
    "        word_frequencies[word] = word_frequencies[word] / maximum_frequency\n",
    "    \n",
    "    # Calculate sentence scores\n",
    "    sentence_scores = {}\n",
    "    for i, sentence in enumerate(sentences):\n",
    "        sentence_scores[i] = 0\n",
    "        for word in sentence:\n",
    "            if word in word_frequencies:\n",
    "                sentence_scores[i] += word_frequencies[word]\n",
    "    \n",
    "    # Sort the sentence scores and get the top N sentences\n",
    "    top_sentences = sorted(sentence_scores, key=sentence_scores.get, reverse=True)[:top_n]\n",
    "    top_sentences = [sentences[i] for i in top_sentences]\n",
    "    \n",
    "    # Flatten the top sentences and join them to form the summary\n",
    "    summary = \". \".join([\" \".join(sentence) for sentence in top_sentences])\n",
    "    \n",
    "    return summary\n",
    "\n",
    "stop_words = set(stopwords.words('english')) \n",
    "\n",
    "def generate_one_word_summary_from_text(text):\n",
    "    sentences = sent_tokenize(text)\n",
    "    \n",
    "    # Preprocess the sentences\n",
    "    preprocessed_sentences = []\n",
    "    for sentence in sentences:\n",
    "        words = word_tokenize(sentence.lower())\n",
    "        tagged_words = pos_tag(words)  # Perform POS tagging\n",
    "        nouns = [word for word, pos in tagged_words if pos.startswith('N')]  # Retain only nouns\n",
    "        filtered_words = [word for word in nouns if word.isalnum() and word not in stop_words]\n",
    "        preprocessed_sentences.append(filtered_words)\n",
    "    \n",
    "    # Calculate sentence scores based on word frequencies\n",
    "    word_frequencies = {}\n",
    "    for sentence in preprocessed_sentences:\n",
    "        for word in sentence:\n",
    "            if word not in word_frequencies:\n",
    "                word_frequencies[word] = 1\n",
    "            else:\n",
    "                word_frequencies[word] += 1\n",
    "    \n",
    "    # Check if word_frequencies dictionary is empty\n",
    "    if not word_frequencies:\n",
    "        return None\n",
    "    \n",
    "    # Sort the word frequencies and select the most frequent word\n",
    "    top_word = max(word_frequencies, key=word_frequencies.get)\n",
    "    \n",
    "    return top_word\n",
    "\n",
    "# Iterate through a0_ to a9_ and generate one-word summaries\n",
    "summary = []\n",
    "for i in range(num_clusters):\n",
    "    set_name = a[i]\n",
    "    reviews_set = set_name \n",
    "    text = \". \".join(reviews_set)\n",
    "    summary.append(generate_one_word_summary_from_text(text))\n",
    "    \n",
    "topic_summary_map = {}\n",
    "\n",
    "# Iterate over the range of available summaries\n",
    "for i in range(min(len(summary), 100)):\n",
    "    topic_summary_map[i] = summary[i]\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data has been exported to data_ready.txt\n"
     ]
    }
   ],
   "source": [
    "# Specify the file path where you want to save the text file\n",
    "output_file_path = \"data_ready.txt\"\n",
    "\n",
    "# Open the file in write mode and write each processed document on a new line\n",
    "with open(output_file_path, \"w\", encoding=\"utf-8\") as output_file:\n",
    "    for document in data_ready:\n",
    "        processed_text = \" \".join(document)  # Join the words in the document list\n",
    "        output_file.write(processed_text + \"\\n\")  # Write the processed text to the file\n",
    "\n",
    "print(\"Data has been exported to\", output_file_path)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "test =pd.DataFrame(new_df['Comments'])\n",
    "test['Aspect Pred'] = pred_df['topic'].map(topic_summary_map)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.sentiment import SentimentIntensityAnalyzer\n",
    "sia = SentimentIntensityAnalyzer()\n",
    "\n",
    "# Function to perform lexicon-based sentiment analysis\n",
    "def perform_sentiment_analysis(text):\n",
    "    sentiment_score = sia.polarity_scores(text)\n",
    "    compound_score = sentiment_score['compound']\n",
    "    \n",
    "    if compound_score >= 0.05:\n",
    "        return 'positive'\n",
    "    elif compound_score <= -0.05:\n",
    "        return 'negative'\n",
    "    else:\n",
    "        return 'neutral'\n",
    "\n",
    "\n",
    "test['sentiment'] = test['Comments'].apply(perform_sentiment_analysis)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Master",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
